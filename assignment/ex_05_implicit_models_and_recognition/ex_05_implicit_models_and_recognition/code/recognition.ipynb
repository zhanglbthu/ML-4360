{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0yBtNVqBQsuX"
      },
      "source": [
        "# Computer Vision Lecture - Exercise 5 Part 2 - Recognition\n",
        "In this exercise, you will gain hands-on experience regarding object recognition. More specifically, we will develop our own simple person detector using methods from classical image processing!\n",
        "\n",
        "This notebook guides you through the relevant steps. When you see helper functions, you don't need to do anything - they are already implemented. The functions you need to implement are indicated as Exercise Function. Sometimes, you can find Hints - these are written upside-down so you can first try to find the solution without reading them.\n",
        "\n",
        "Good luck and lot's of fun!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TAF5mRfdT5EF"
      },
      "source": [
        "## Preliminaries\n",
        "Let's first install dependencies and import the required libaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9CpnEyzQnbs"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu --no-cache --quiet\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import tqdm\n",
        "from skimage.feature import hog\n",
        "from sklearn.svm import SVC\n",
        "import faiss      # fast kNN search\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# set random seed for reproducability\n",
        "np.random.seed(42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5C1GFSiIj_v_"
      },
      "source": [
        "First, we load a the [Penn-Fudan Pedestrian Dataset](https://www.cis.upenn.edu/~jshi/ped_html/) and visualize some of its images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IWx9znwp4x9"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "  ''' Penn-Fudan Pedestrian dataset.\n",
        "  \n",
        "  Args:\n",
        "  root (str): path to data directory\n",
        "  split (str): dataset split, \"train\" or \"val\"\n",
        "  '''\n",
        "  def __init__(self, root, split='train'):\n",
        "    self.root = root\n",
        "    \n",
        "    # download dataset if needed\n",
        "    if not os.path.isdir(self.root):\n",
        "      !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip --directory-prefix=$self.root\n",
        "      !unzip -qq $self.root/PennFudanPed.zip -d $self.root\n",
        "\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    imgs = list(sorted(os.listdir(os.path.join(root, \"PennFudanPed\",\"PNGImages\"))))\n",
        "    masks = list(sorted(os.listdir(os.path.join(root, \"PennFudanPed\", \"PedMasks\"))))\n",
        "\n",
        "    # split into train and validation set\n",
        "    ntrain = int(0.8*len(imgs))\n",
        "    if split == 'train':\n",
        "      self.imgs = imgs[:ntrain]\n",
        "      self.masks = masks[:ntrain]\n",
        "    elif split == 'val':\n",
        "      self.imgs = imgs[ntrain:]\n",
        "      self.masks = masks[ntrain:]\n",
        "    else:\n",
        "      raise AttributeError('split must be \"train\" or \"val\".')\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load images and masks\n",
        "    img_path = os.path.join(self.root, \"PennFudanPed\", \"PNGImages\", self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, \"PennFudanPed\", \"PedMasks\", self.masks[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    # note that we haven't converted the mask to RGB,\n",
        "    # because each color corresponds to a different instance\n",
        "    # with 0 being background\n",
        "    \n",
        "    mask = Image.open(mask_path)\n",
        "    # convert the PIL Image into a numpy array\n",
        "    mask = np.array(mask)\n",
        "    # instances are encoded as different colors\n",
        "    obj_ids = np.unique(mask)\n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # split the color-encoded mask into a set\n",
        "    # of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    # get bounding box coordinates for each mask\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "        pos = np.where(masks[i])\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    return img, {'boxes': boxes}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuSihB2Ek3f3"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "def get_width_and_height(box):\n",
        "  \"\"\" Helper function to determine size of bounding boxes.\n",
        "\n",
        "  Args:\n",
        "  box (iterable): (left, upper, right, lower) pixel coordinate\n",
        "  \"\"\"\n",
        "  return box[2]-box[0], box[3]-box[1]\n",
        "\n",
        "def draw_box(ax, box, color='r'):\n",
        "  ''' Plot box on axes.\n",
        "  \n",
        "  Args:\n",
        "  ax (matplotlib.axes.Axes): axes to add box to\n",
        "  box (iterable): (left, upper, right, lower) pixel coordinate\n",
        "  color (str or list): edgecolor of the box\n",
        "  '''\n",
        "  anchor = box[:2]\n",
        "  W, H = get_width_and_height(box)\n",
        "  patch = Rectangle(anchor, width=W, height=H, edgecolor=color, facecolor='none')\n",
        "  ax.add_patch(patch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "5LVXbrial9_x",
        "outputId": "2b880629-edfb-426a-8d8b-cbbc06cc92f3"
      },
      "outputs": [],
      "source": [
        "train_set = PennFudanDataset('data', split='train') \n",
        "for i in range(2):\n",
        "  img, annotation = train_set[i]\n",
        "  fig, ax = plt.subplots(1)\n",
        "  ax.set_axis_off()\n",
        "  ax.imshow(img)\n",
        "  for box in annotation['boxes']:\n",
        "    draw_box(ax, box)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPjCREknT8m"
      },
      "source": [
        "## Developing a Person Detector\n",
        "\n",
        "Okay, we have our dataset so let's start to program our person detector! We will do this using a sliding window approach. The steps that we have to take are:\n",
        "\n",
        "1.   Get **positive** training examples, i.e. image patches that contain a person.\n",
        "2.   Get **negative** training examples, i.e. image patches that do not contain a (complete) person.\n",
        "3.   Extract Histogram of Oriented Gradients (HOG) **features** to obtain a more robust image descriptor than the raw pixel values.\n",
        "4.   **Train our preferred classifier** with this training data. We will implement a simple nearest neighbor search and a Support Vector Machine.\n",
        "5.   Extract the image patches and HOG features from our validation image using a **sliding window approach and evaluate our classifier** on each patch to detect persons in the validation image."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D7s84mv1rJ2G"
      },
      "source": [
        "### Getting the Training Patches\n",
        "\n",
        "To obtain positive training examples, we can simply crop the training images according to the given bounding boxes.\n",
        "We also resize the crops to a given size to ensure that all of our patches have the same size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCKwGBoTra6m"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "def get_resized_patch(img, box, patch_size):\n",
        "  ''' Crop patch from image and resize it to given size.\n",
        "  \n",
        "  Args:\n",
        "  img (PIL.Image.Image): image\n",
        "  box (iterable): (left, upper, right, lower) pixel coordinate\n",
        "  patch_size (tuple): width, height of resized patch\n",
        "  '''\n",
        "  assert isinstance(img, Image.Image), 'img needs to be PIL.Image.Image'\n",
        "  crop = img.crop(box)  \n",
        "  patch = crop.resize(patch_size)\n",
        "  return patch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8alWX7o5rvH1"
      },
      "source": [
        "Next, we need to get negative examples for training. Complete the function below, such that it places a box of a given size at a random location in the image. Ensure that the whole box is within the image boundaries. The function should return the (left, upper, right, lower) pixel coordinates of the box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8wOiGllumIr"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "#### Exercise Function ####\n",
        "###########################\n",
        "def get_random_box(boxsize, imsize):\n",
        "  \"\"\" Returns randomly located box with same size as box. \n",
        "  \n",
        "  Args:\n",
        "  boxsize (tuple): width, height of box\n",
        "  imsize (tuple): width, height of image / image boundaries\n",
        "  \"\"\"\n",
        "  W, H = imsize\n",
        "\n",
        "  # Insert your code here\n",
        "  \n",
        "  assert all(b >= 0 for b in box) and (box[2] < W) and (box[3] < H), f'Box {box} out of image bounds {W, H}.'\n",
        "  return box"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tMP-U_VMvM2a"
      },
      "source": [
        "In addition to these rather simple negative examples, let's also include some more challenging negative examples to improve the quality of our classifier later on. We construct hard negative examples by adding a small offset to the ground truth boxes. Thereby, the negative examples are close to positive detections.\n",
        "Complete the function below, such that it adds a small random offset to a given box. The minimal offset in height/width should be 1 pixel and the maximal height/width offset should not exceed 20% of the box height/width."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3rPRxV9w0lE"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "#### Exercise Function ####\n",
        "###########################\n",
        "def add_offset_to_box(box, imsize):\n",
        "  \"\"\" Add a small random integer offset to the box.\n",
        "\n",
        "  Args:\n",
        "  box (iterable): (left, upper, right, lower) pixel coordinate\n",
        "  imsize (tuple): width, height of image / image boundaries\n",
        "  \"\"\"\n",
        "  W, H = imsize\n",
        "\n",
        "  # Insert your code here\n",
        "\n",
        "  # ensure to stay within image boundaries / do not mind changing size slightly\n",
        "  off_box[0] = max(0, off_box[0])\n",
        "  off_box[1] = max(0, off_box[1])\n",
        "  off_box[2] = min(W-1, off_box[2])  \n",
        "  off_box[3] = min(H-1, off_box[3])\n",
        "\n",
        "  assert all(b >= 0 for b in off_box) and (off_box[2] < W) and (off_box[3] < H), f'Box {off_box} out of image bounds {W, H}.'\n",
        "\n",
        "  return off_box"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QRc7nFv8x5gE"
      },
      "source": [
        "With these tools at hand, we can collect our training patches from the images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7eXTzDNtccm"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "def imgs_to_patches(train_set, patch_size, n_random_negatives=3, n_hard_negatives=3):\n",
        "  positives = []\n",
        "  negatives = []\n",
        "  orig_sizes = []\n",
        "  for img, annotation in tqdm(train_set):\n",
        "    for box in annotation['boxes']:\n",
        "      \n",
        "      # keep track of original box sizes for sliding window approach later\n",
        "      orig_sizes.append(np.array(get_width_and_height(box)))\n",
        "\n",
        "      # extract positives\n",
        "      positives.append(get_resized_patch(img, box, patch_size))\n",
        "\n",
        "      # add some random negatives\n",
        "      for _ in range(n_random_negatives):\n",
        "        boxsize = get_width_and_height(box)\n",
        "        mod_box = get_random_box(boxsize, img.size)\n",
        "        negatives.append(get_resized_patch(img, mod_box, patch_size))\n",
        "\n",
        "      # add some hard negatives by adding noise on box\n",
        "      for _ in range(n_hard_negatives):\n",
        "        mod_box = add_offset_to_box(box, img.size)\n",
        "        negatives.append(get_resized_patch(img, mod_box, patch_size))\n",
        "  \n",
        "  return positives, negatives, orig_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6nNvzsVz-qL",
        "outputId": "951605b2-70d7-4eed-b8b0-7f5fd9e04628"
      },
      "outputs": [],
      "source": [
        "patch_size = (50, 150)          # hyperparameter\n",
        "positives, negatives, orig_sizes = imgs_to_patches(train_set, patch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_5K1WFz_zyIa"
      },
      "source": [
        "Let's take a look at some of the positive and some of the negative patches to ensure everything is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "4CA1wEuE8wil",
        "outputId": "d78091c3-71ed-4b86-8210-eb47d46b4855"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].set_axis_off()\n",
        "ax[0].imshow(np.concatenate([np.array(p) for p in positives[:5]], 1))\n",
        "\n",
        "ax[1].set_axis_off()\n",
        "ax[1].imshow(np.concatenate([np.array(n) for n in negatives[:5]], 1))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "izgoBht23GG_"
      },
      "source": [
        "Do both positive and negative samples look reasonable? If they do, we can advance with the next step."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h5_6GrEk6VfA"
      },
      "source": [
        "### Extracting HOG features\n",
        "\n",
        "In the next steps, we convert the RGB pixel values from the image patches to more robust HOG features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "dUCPzm2JwIO2",
        "outputId": "64bdf8e6-94d2-492b-8f59-4a0ac004d622"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "def get_hog(img, **kwargs):\n",
        "  \"\"\" Extract HOG features with predefined settings.\n",
        "\n",
        "  Args:\n",
        "  img (np.ndarray): image with channel dimension last (HxWx3)\n",
        "  \"\"\"\n",
        "  return hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), channel_axis=2, **kwargs)\n",
        "\n",
        "# visualize HOG for one image\n",
        "img = np.array(positives[0])\n",
        "fd, hog_image = get_hog(img, visualize=True)      # if visualize=True this function returns the descriptors + the image, otherwise it only returns the descriptors\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(img)\n",
        "ax[1].imshow(hog_image, cmap=plt.cm.gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL3R-e0P9aQx",
        "outputId": "2252cc93-6c14-42f0-f4cf-4a8f1e9845b4"
      },
      "outputs": [],
      "source": [
        "fds_pos = []\n",
        "for p in tqdm(positives, position=0, leave=True, desc='Extract HOG features for positive samples'):\n",
        "  fds_pos.append(get_hog(np.array(p)))\n",
        "\n",
        "fds_neg = []\n",
        "for n in tqdm(negatives, position=0, leave=True, desc='Extract HOG features for negative samples'):\n",
        "  fds_neg.append(get_hog(np.array(n)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rc4KkuC96aqS"
      },
      "source": [
        "### Training the classifiers\n",
        "\n",
        "First, we construct our training set using the HOG feature descriptors from the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr5VUZlT7aV4"
      },
      "outputs": [],
      "source": [
        "X = np.stack(fds_pos + fds_neg)\n",
        "y = np.concatenate([np.ones(len(positives), dtype=bool), np.zeros(len(negatives), dtype=bool)])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HE0nZtcN7Rsz"
      },
      "source": [
        "First, we train a simple nearest neighbor classifier. Since the sliding window approach will give us a lot of patches per image, a naive nearest neighbor approach can be very slow. Therefore, we use the [Faiss Library](https://github.com/facebookresearch/faiss) which is optimized for exactly such applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJXZfLqP8Vpm"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "def init_nn_search(X):\n",
        "  \"\"\" Initialize the index for a nearest neighbor search.\n",
        "\n",
        "  Args:\n",
        "  X (np.ndarray): training data\n",
        "  \"\"\"\n",
        "  d = X.shape[1]\n",
        "  quantizer = faiss.IndexFlatL2(d)                                  # measure L2 distance\n",
        "  index = faiss.IndexIVFFlat(quantizer, d, 100, faiss.METRIC_L2)    # build the index\n",
        "\n",
        "  index.train(X.astype(np.float32))\n",
        "  index.add(X.astype(np.float32))                                   # add vectors to the index\n",
        "  return index\n",
        "\n",
        "index = init_nn_search(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-zF-Nk3k9Qt3"
      },
      "source": [
        "Second, we train a support vector machine using `sklearn.svm.SVC`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkoUIzEH9QBA"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "##### Helper Function #####\n",
        "###########################\n",
        "def train_svm(X, y):\n",
        "  \"\"\" Train a support vector machine.\n",
        "\n",
        "  Args:\n",
        "  X (np.ndarray): training data\n",
        "  y (np.ndarray): training labels\n",
        "  \"\"\"\n",
        "  clf = SVC(class_weight='balanced')        # use balanced weight since we have more negatives than positives\n",
        "  clf.fit(X, y)\n",
        "  return clf\n",
        "\n",
        "svm = train_svm(X, y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q-R9We_8_N7z"
      },
      "source": [
        "### Evaluating the classifiers\n",
        "\n",
        "In order to evaluate our classifiers, we need to extract the image patches and their HOG features from the target image, so that our target data is similar to our training data. \n",
        "We do this using a sliding window approach. \n",
        "Implement a function that extracts all crops of a given window size from an image in a sliding window manner. Similar to the training data, resize the crops to a given size and convert these patches to HOG features.\n",
        "Your function should return two `numpy.ndarray`s. The first contains the feature descriptors of all patches in the image (Pxd), and the second contains the corresponding anchor, i.e. the (upper,left) pixel coordinate, of each patch (Px2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLGzjjepDOKP"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "#### Exercise Function ####\n",
        "###########################\n",
        "def img_to_hog_patches(img, window_size, patch_size, step_size=1):\n",
        "  \"\"\" Extract hog feature patches from an image using a sliding window approach.\n",
        "\n",
        "  Args:\n",
        "  img (PIL.Image.Image): image\n",
        "  window_size (tuple): width, height of window\n",
        "  patch_size (tuple): width, height of resized patch\n",
        "  step_size (int): step size of window (for faster evaluation)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Insert your code here\n",
        "  \n",
        "  # convert to numpy arrays\n",
        "  fds = np.stack(fds)\n",
        "  anchors = np.stack(anchors)\n",
        "  return fds, anchors"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sXAZ5Oi_E7s3"
      },
      "source": [
        "Now, let us evaluate our classifiers on some new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "lPDFo1HSAK6W",
        "outputId": "8bb1ff70-5f69-4309-8f6d-c6a2c38463d6"
      },
      "outputs": [],
      "source": [
        "val_set = PennFudanDataset('data', split='val') \n",
        "\n",
        "N_imgs = 3\n",
        "window_size = np.stack(orig_sizes).mean(0).astype(np.uint)      # use average size of training boxes as window size\n",
        "\n",
        "for i in range(N_imgs):  \n",
        "  img, ann = val_set[i]\n",
        "  fds, anchors = img_to_hog_patches(img, window_size, patch_size, step_size=10)\n",
        "\n",
        "  # evaluate NN search\n",
        "  _, I = index.search(fds.astype(np.float32), 1)        # search nearest neighbor for each grid point\n",
        "  is_positive = y[I]                                    # assign labels of training points\n",
        "  \n",
        "  mask = is_positive.reshape(-1, 1).repeat(2, 1)        # convert to mask for anchors\n",
        "  anchors_nn = anchors[mask].reshape(-1, 2)\n",
        "\n",
        "  # evaluate SVM\n",
        "  k = 5\n",
        "  scores = svm.predict(fds)\n",
        "  idcs_sorted = np.argsort(scores)[::-1][:k]                # sort get top k predictions\n",
        "  anchors_svm = anchors[idcs_sorted]\n",
        "\n",
        "  # visualize the results\n",
        "  fig, ax = plt.subplots(2)\n",
        "  \n",
        "  ax[0].imshow(img)\n",
        "  for a in anchors_nn:\n",
        "    box = (a[0], a[1], a[0]+window_size[0], a[1]+window_size[1])\n",
        "    draw_box(ax[0], box, color='r')\n",
        "  \n",
        "  ax[1].imshow(img)\n",
        "  for j, a in enumerate(anchors_svm):\n",
        "    box = (a[0], a[1], a[0]+window_size[0], a[1]+window_size[1])\n",
        "    color = 'r' if j == 0 else 'orange'\n",
        "    draw_box(ax[1], box, color=color)\n",
        "  \n",
        "  plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A2vXs2bXJLI0"
      },
      "source": [
        "Great job! While not perfect, HOG features combined with a simple classifier yield reasonable detections for this dataset. \n",
        "You now gained hands-on experience in the fields of image detection!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
